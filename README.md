# Propaganda-detection-
## Introduction:
This project tackles the complex task of classifying text propaganda to identify and categorize various forms of propaganda within textual data. Given the intricate nature of this classification challenge, it necessitates a nuanced understanding of natural language processing (NLP).

## Approach: 
Our project employs a sophisticated and multifaceted approach to address this task effectively. We use classification techniques, word embeddings, and neural language models to accurately identify and classify text propaganda.

## Methods Used:

### Word Embeddings: 
We harnessed the power of Word2Vec embeddings to capture semantic relationships between words in the text data. GloVe (Global Vectors for Word Representation) embeddings were employed to represent words in a vector space, enhancing our grasp of word semantics.

### Neural Language Models:
We implemented a GRU Ensemble (Gated Recurrent Unit) to capture sequential dependencies within the text data. Furthermore, we utilized BERT (Bidirectional Encoder Representations from Transformers), a formidable transformer-based model, to extract contextual information within the text, improving classification accuracy.

### Traditional Machine Learning:
As a baseline, we applied logistic regression to compare the performance of our deep learning models against a traditional machine learning approach.

### Evaluation:
Rigorous evaluations were conducted to assess the effectiveness of our diverse approaches in classifying text propaganda. This evaluation process enabled us to leverage the strengths of both traditional machine learning and cutting-edge deep learning techniques.

For comprehensive project details, you can go ahead and explore the project structure, data, notebooks, trained models, and evaluation results available in this repository.
